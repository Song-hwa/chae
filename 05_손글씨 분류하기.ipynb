{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손글씨 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "((X_train, y_train), (X_test, y_test)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f859c84da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#cmap = plt.cm.binary : 색상> 흑백\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0~255 까지의 숫자로 ㅇㅣ루어져있음\n",
    "#0이 흰색\n",
    "#255가 검은색\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기계는 0과 1사이의 숫자를 좋아함\n",
    "#0~255까지의 숫자를 0~1까지로 만들어줌\n",
    "#전체 데이터를 255로 나눠줌\n",
    "X_train[0]\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28X28의 2차원 데이터를 784의 1차원 데이터로 만들어줄 필요가 있음\n",
    "#input_dim에 집어넣기 위해서\n",
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_test = X_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력층 개수 : 784\n",
    "#출력층 개수 : 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1f857aae198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#모델 설계\n",
    "model = Sequential()\n",
    "#입력층, 중간층의 활성화 함수 : sigmoid\n",
    "model.add(Dense(units = 32, input_dim = 784, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 64, input_dim = 784, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 128, input_dim = 784, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 64, input_dim = 784, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 32, input_dim = 784, activation = 'sigmoid'))\n",
    "\n",
    "#출력층의 활성화 함수 : sigmoid\n",
    "model.add(Dense(units = 10, activation = 'sigmoid'))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 46,218\n",
      "Trainable params: 46,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 방법 설정\n",
    "# loss = categorical_crossentropy\n",
    "#optimizer = adam\n",
    "#metrics = accuracy\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.3946 - accuracy: 0.4700\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.6510 - accuracy: 0.7942\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.3758 - accuracy: 0.9044\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2915 - accuracy: 0.9235\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2505 - accuracy: 0.9327\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2247 - accuracy: 0.9399\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2029 - accuracy: 0.9453\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1837 - accuracy: 0.9505\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1680 - accuracy: 0.9543\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1533 - accuracy: 0.9584\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1416 - accuracy: 0.9617\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1328 - accuracy: 0.9632\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1236 - accuracy: 0.9667\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1168 - accuracy: 0.9681\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1112 - accuracy: 0.9693\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1046 - accuracy: 0.9713\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0992 - accuracy: 0.9725\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0955 - accuracy: 0.9739\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0908 - accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0882 - accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "#모델 학습\n",
    "#20\n",
    "history1 = model.fit(X_train,y_train,epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1888 - accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18875090100094677, 0.9534]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 평가\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 조건은 동일\n",
    "#model2라는 딥러닝 모델 설계\n",
    "#입력층과 중간층의 활성화함수 sigmoid > relu\n",
    "#history2 = model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "#입력층, 중간층의 활성화 함수 : relu\n",
    "model2.add(Dense(units = 32, input_dim = 784, activation = 'relu'))\n",
    "model2.add(Dense(units = 64, activation = 'relu'))\n",
    "model2.add(Dense(units = 128,  activation = 'relu'))\n",
    "model2.add(Dense(units = 64,  activation = 'relu'))\n",
    "model2.add(Dense(units = 32,  activation = 'relu'))\n",
    "\n",
    "#출력층의 활성화 함수 : softmax\n",
    "model2.add(Dense(units = 10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6  7  8  9\n",
       "0      0  0  0  0  0  1  0  0  0  0\n",
       "1      1  0  0  0  0  0  0  0  0  0\n",
       "2      0  0  0  0  1  0  0  0  0  0\n",
       "3      0  1  0  0  0  0  0  0  0  0\n",
       "4      0  0  0  0  0  0  0  0  0  1\n",
       "...   .. .. .. .. .. .. .. .. .. ..\n",
       "59995  0  0  0  0  0  0  0  0  1  0\n",
       "59996  0  0  0  1  0  0  0  0  0  0\n",
       "59997  0  0  0  0  0  1  0  0  0  0\n",
       "59998  0  0  0  0  0  0  1  0  0  0\n",
       "59999  0  0  0  0  0  0  0  0  1  0\n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.4116 - accuracy: 0.8763\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1938 - accuracy: 0.9457\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1590 - accuracy: 0.9541\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1394 - accuracy: 0.9603\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1256 - accuracy: 0.9640\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1139 - accuracy: 0.9670\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1011 - accuracy: 0.9708\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0935 - accuracy: 0.9717\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0867 - accuracy: 0.9745\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0827 - accuracy: 0.9749\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0770 - accuracy: 0.9763\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0729 - accuracy: 0.9778\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0686 - accuracy: 0.9793\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0671 - accuracy: 0.9796\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0625 - accuracy: 0.9807\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0608 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0574 - accuracy: 0.9821\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0542 - accuracy: 0.9828\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0525 - accuracy: 0.9835\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0499 - accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train,y_train, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.1592 - accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15923973087593912, 0.9646]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 > 입력층, 중간층 활성화함수 : sigmoid\n",
    "#model 2 > 입력층, 중간층 활성화 함수 : relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAz90lEQVR4nO3deXQc5Z3v//fT3Vqs1Ysk490GC2wDxjbGMeCwOXbYlywDhIyBQIBhyA3MyZxhZs41y+QXyC+ThEzghus7wwCBAAMhxASzxDDEgRsCBmQCGMnC2Fi2JcuSZbWstbuf+0eVpFarJbWklkrq/rzO6dO1PF397XJL+vipp6qMtRYRERERGRqf1wWIiIiIjGcKUyIiIiLDoDAlIiIiMgwKUyIiIiLDoDAlIiIiMgwKUyIiIiLDEPDqjYuKiuzcuXO9ensRERGRhL377rsHrbXF8dYNGKaMMQ8BFwIHrLUnxFlvgJ8B5wPNwDXW2vcG2u7cuXPZunXrQM1EREREPGeM2d3XukQO8z0MnNvP+vOAUvdxA/CLwRQnIiIiMp4NGKastVuA+n6aXAI8ah1vARONMdOSVaCIiIjIWJaMAegzgD1R81XuMhEREZGUl4wwZeIsi3vDP2PMDcaYrcaYrbW1tUl4axERERFvJSNMVQGzouZnAvviNbTWbrDWLrfWLi8ujjsgXkRERGRcSUaY2gisM46VwGFr7f4kbFdERERkzEvk0ghPAGcBRcaYKuAOIAPAWvsgsAnnsgiVOJdGuHakihUREREZawYMU9baKwdYb4G/TVpFIiIiIuOIbicjIiIiMgye3U5GRERE0oC1YCMQCTmPcAdEwt3z8R7hftbFa1N0LMw51bOPqDAlIiKSiB6hICoMRAeFrnXuehuOad/X68Ju20jU68JRz5GYebdtj/nQINr2s7xXm/62McB7RUIQ6Rj5f5vl1ylMiYjIKLPWeWC7QwIxywZ6jp7u/KMZdh+d0109EQOtC0G4vXs60hF/XZ9/7IcZIPp6fXRIsmHv/r0GYvzgC4DP70773Gd/zHOc5b3auNsKZMXZhi9qPhBnWUxbf4bbLt7DH7O+c5sZMfPuw9/XdgKQlefp7leYEhFJlkjE+aMfboOQ+xxu757ueu5cHv3clli7rrbxthGzrUiIuEFovOj8w+p3H11/ZAcTFBIJBQO8PvaPuvHFCQDuc3TIiH6diZmPfl0i4WaggCSeUpgSkfGlsxekKzh0uAGmPWZZdJCJeXQtc9t2BpH+wkl0u9jw07kumYczjN8JAf5M9zkLApkxz1mQXdh3G18AMGDMwM+JtOlq6+u9zOd36vBluD0IUQGoaz4zZl0gqk2cdSbeDTZkPLDWErHOs8XtyMQ6eT5m3tKzHf2ss87KXtvMzQxQmJPh0adVmBJJb6F2aG+CtqD73ATtQffZnW8L9lwWanUOiXQeGup6jkQdKopEretv/QBtOg8LxYaZZPeuDBhc3EdObh/r+gg6/ow4yzITC0k+f3I/o6QUay0dYUtHOEJ7KEJHOEJbKEJ7ONJ7WShCR9h2LWuPWt/Ztt3dVkePbVhnOu7r+tqe7Xq9HcVO0G+unM33Lz1x9N4whsKUiFdix2PEDk5NaN5dFm6H9iO9g1CP+WDvZeH2xGr1Z0JmnjMuITDB/UPv9lB09lT0mvZ1t/H5wWT0vb7rtabnOn9Gd5CJDif+TGc6dll0uOl8beyyziAT/dBhEsEJKG2hCG0dEVpDYTdkRAi5QSMUsV0BJBS2hCJO4AhFutt0uMs7wpZQONJjmdOmc1vRbWyPQBIdhHqGFdtVU3sokvTPn+n3keE3ZAR87rSPzICzzHl2HrlZAXfakBnwO8892vvI9Bv8Pp/zI03nj7fT0+gsMz3XufPOehO13J13p+mxrnv5/BKNmRIZ+6yFjhZoOQStDc5zr0fM8tYG99BPnDN8OseyjBTjc8NPfncIysyD3OKe81l5kJnfz7z7+kDmyNUqEiW6xyUU1csRigocraEwrR1hJ/R0hN15dzpqWdf6jjAtPdZFaHOXt3ZEbS80cr0pfp8h4HNCR8BvCPh9ZPic4BLwGTL8PrIC3YEkNytAZsCZzvS74SZgyPT73WXdAaezXY9tRAebQPS8iZn3dQUjo8OqQ6YwJeklEoG2xt5BqCsgNfQdjsJtfW/XF4AJk7ofBdOhZJF7uCYQZ9BpovPxzmDp4zWZUYEoI0fjTaSHsNur0uOQTMjSHg7THupe1xGK0NZ1aMdZ3xGy/R4aij7E0xHuPR0bijrCEToizvvHtg9FkpNmsgI+sjP8ZGe4zwFnOivDT+GEDLLzs3quz/CTHXDWdy7vDBwBnxOAMvxO6An4nPAR6OzJ8XcHogw3LGV0vcZZ5/Pp5zGVKUzJ2GctdDR3j99pa4w6bBXzGGh5e1P/75WRGxWKJkLR/J4hacIkyJ7Ye1lmrsKLjIhQOEKwNURjaweNLc7z4ZYOGls6eixz5kM91rW0h90QYwknKaREy+w6HGR69HB0TQfc3he/j4LMjK7p6OUZgT5e2xlE3ENG0dNZUeEoOyr8OMv8ZAV8Ci8yqhSmZOSF2pxentbDbg9QQ9Szu6y1ISr8RAWidvfZJjA+wBdwD0vlO89Z+ZAzGSbN6bk8u6B3GOoMSTqcJcNgrSUUcce1uGNb2joitIedQ0htIecwU48A5Iag2HDUGYqOtPd/bSOfgYIJGRRkZ1AwIUBBdgbzS/LIzw6QkxnoPrTj97uHiaLHtfjc8TE9x8T0PgxkooJTZ2+NDguJdFKYkoF19gwNFIj6WhZq6X/7GbnO6d1ZUSEofypkFUSN+cnv/ehaXuAOjM5W75DEFYlYgm0hgq0dBFtD7qOjq8cn2BrqOuuprXPgcaj7TKjOQ1yd69qi14ed8TedbQbbAWQM5GcFegSi2ZNzKJiQQWFMSHLaBLrXTcggN9OvUCPiMYWpdNfaCI37oLEKDu+Fxr3dz437oKXeCUQDXT8nq8A9/FXoPBfNd56zC53DZZ2HxrInuvOF3evVGyT9iEQsTe09A1BXEHJ7deKtiw5KTW2hhN4rM+Ajy+8jK2q8TGbAR1bA39Uzk5MTICvQc13XvDsAODMQ/7Wd43iiw1F+VkCHpETGOYWpVNbe7IajqpiQFDXd1hjzIgN5U6FwBhQfCzlTogLQxJhwFLVM18SRAYTCEQ63dNDQ0kFDczsNzR0cau6ebmhp51BzB4ebOzjkLmts6aCpPTTgGVaZfh/52QH3kUF+doC5RTnkZzs9O53ruqed54IJznNelhOQ1MMjIkOhMDVedbT2DkbR04ernMNssXKLoWAGTDkG5p3hnHVWONNZVjgD8o5ST5H0KxKxNLZ2uGGovVc4OtzshKKey9sJtvbdO+QzMDEnk4kTMpiYk8HUgmyOm5rfdVgr3z3UlR8ThjoDkoKQiHhJYWosa2+G+k/h4A6o+xTqdjjTDZ9D88He7SdMdgJR4SyYvdIJSgUznWUFM5z5QNbofw4Zk9pDERpa2mlscYJRgxuADrc4gaihJf6yxpaOPscFGQMF2U4gmpiTyaScTI4uynWCUk4GEydkMCk3k8IJGUzqXJaTqUNdIjKuKUx5LRKBw3ugrtJ5HNzhhKa6T53l0QpnOT1KCy+KCkgz3J6l6ZAxwZvPIJ5q7QhTd6SdhuZ2Dnf1CHW4h9TcZe5htMMtoa5Q1NzPWWLGQOEEJ/wUTsigMCeTOZNzmJjjzDtBqTs0TXTDUcGEDPwKRSKSZhSmRktLQ0xYqoSDlU7PU6i1u11WAUyZD3NOgymlTngqKoXJx0Bmjmfly+gKRyx1R9qoDbZxsKmd2mBb96Opjdpga9d8Yz+HzzL9PjfwOCFoxsQJHD+9oOtwWmdQip6fOCGT/Gz1FImIJEphKplC7XBoV/fhuOjepujDcsYPk+Y6IemYs53nKaVOiMor0en9KcpaS2NrKCYUxYYk51F/pC3uobTcTD/F+VkU52dx3FH5rJpfRHF+FkV5Wd2H0qJCUXaGxhKJiIw0halk2P472HwH1H8GNurQSW6xE5KOO687MBWVOkHKn+FZuZJc7aEIB5vaOOAGoQPBVg40xg9L8W5OmuE3FOc5AWnGxGyWzCrsmu965GVTlJ9JTqZ+ZEVExhr9Zh6u/R/Ar6+HyfNg1W1O71KR28s0YaLX1ckQWWtpagtFBaQ2DjS2OgGpsWdwOtQc/xpcU3Izu8LQ0cW5PQNS1HThhAz1HomIjGMKU8NxpA6evMq5GOW63zqH6GRMi0QsB490B6RatwfpQGNrj+BUG2yjpaP3AO1Mv68rBM2ZksMp8yZRnJdNSUEWJe7ykvxspuRlkuH3efAJRURktClMDVU4BM9cA001cO2LClJjyOHmDj6vb2bPoWb21De70y3sqW9m76EW2sO9D7XlZwcocYPQklkTnemC7nBUol4kERHpg8LUUG2+Az7bApc8ADNP9rqatNLaEWZvQwuf1zdT5Qalz+uc8PR5fXOvi0NOzMlg1qQcFk7LZ+3xU5kxcUKPXqTi/CyyM3QFdxERGRqFqaH44Gn40/2w4gZY+k2vq0k5kYilJtjKnnonMO3pfBxqZk99C9WNrT3aZwZ8zJo0gVmTczh5ziRmTcph1mRnftbkHAqyNdhfRERGjsLUYO3fBhtvgTmnw5d/4HU145a1lgPBNnbUNLHjQJBPa5v4vL6FqvpmqmIOxRkD0wqymTk5h9PnFzF7shOWZrthqTgvS9dEEhERzyhMDcaROnjym87Nf7/+iC5vkABrLfsPt7LjQBM7aoJd4WnHgaYeh+MKsgPMmZLLgmn5rFk0tatXafbkHKZPzCYroMNwIiIyNilMJSp6wPm3XoK8Yq8rGlMiEcvehhYnKNU0OeHpQBOVNUGORN22ZEpuJvNL8rhkyXRKS/IpnZpHaUk+RXmZGtgtIiLjksJUon6/3hlwfukvYMYyr6vxTDhi2VPf7IalIJVucKo80NTjUgLF+VmUluTxtZNnUjo1n9KSPOaX5DElTzdaFhGR1KIwlYhtT8FbD8CKG2HJN7yuZtR8XtfMx/sbqXQPy1XUNPFpbVOPq3gfVZBN6dQ8rlwx2+1lckLTxJxMDysXEREZPQpTA9lXBs//D5izCr78/3ldzYjbU9/M8x/s4/lt+9m+v7Fr+YyJEyidmseq+VMoLcln/lQnNOlMORERSXcKU/05chCe+ibkFMHXH07ZAec1ja387oP9PL9tH2V7GgBYOnsi//PCRSyfM4ljSvLIy9JXRUREJB79hexLOARPXwNNB1JywHn9kXZe/NAJUH/+rB5rYdG0Av7h3AVcuHgasybneF2iiIjIuKAw1Zff/0/Y9Ue49MGUGXDe2NrBKx/V8Py2fbxReZBwxHJ0cS7fXV3KhYunM78kz+sSRURExh2FqXi2PQlv/S/4wk2w5EqvqxmW5vYQr24/wPPb9vF6eS3t4QgzJk7g2188motOmsaiaQW6JIGIiMgwKEzF2lcGz3/XGXC+9vteVzMkbaEwWyoO8vy2fWzeXkNze5iS/CyuWjmbi06aztJZExWgREREkkRhKto4HnAeCkf4v5/W8fy2fbz0UTXB1hCTcjK4dOkMLlo8nRXzJuPXLVdERESSTmGqU7jDGXB+pHbcDDiPRCxbdx/i+W372PSX/dQdaScvK8Da46dy0UnTWTW/iAy/z+syRUREUlpCYcoYcy7wM8AP/Lu19t6Y9ZOAh4BjgFbgW9baD5Nc68h6xR1wftn/hulLva6mT9ZaPqg6zPPb9vG7D/ZT3dhKdoaP1QunctHi6Zx1XDHZGbqPnYiIyGgZMEwZY/zAA8AaoAp4xxiz0Vr7cVSzfwLKrLWXGWMWuO1Xj0TBI2Lbk/DnX8AX/gZOusLravr1N4+9x0sfVZPhN5x5bAn/eP4CvrRwKrm6DpSIiIgnEvkLvAKotNbuBDDGPAlcAkSHqUXAPQDW2k+MMXONMVOttTXJLjjp9r3vDDif+0VY+y9eV9Ovt3bW8dJH1Vy/ah7fOaeUwpzxM6ZLREQkVSUyoGYGsCdqvspdFm0b8BUAY8wKYA4wMxkFjqimWnjym5BbPC4GnP/09xWU5GfxvS8fpyAlIiIyRiQSpuKdAmZj5u8FJhljyoDvAO8DoV4bMuYGY8xWY8zW2trawdaaXJ0DzpsPwuWPQW6Rt/UM4P9+epA/f1bPzWcdozFRIiIiY0gih/mqgFlR8zOBfdENrLWNwLUAxrmA0Wfug5h2G4ANAMuXL48NZKPr5X+G3W/AZRtg+hJPSxmItZaf/r6CowqyuWLFbK/LERERkSiJ9Ey9A5QaY+YZYzKBK4CN0Q2MMRPddQDXA1vcgDU2vf84vP2/YeXNcNLlXlczoDcr63hn1yFuPlu9UiIiImPNgD1T1tqQMeYW4GWcSyM8ZK39yBhzk7v+QWAh8KgxJowzMP26Eax5ePa+C7+7zRlwvmZsDzgHt1dqcwXTCrO5/JRZA79ARERERlVC59NbazcBm2KWPRg1/SegNLmljYCmA/DUX0NeiTvgfOxfTuCPOw7y7u5D/MulJ5AVUK+UiIjIWDP200SydA04r4NvvTzmB5xDd6/U9MJs/mr52D85UkREJB2lz71GXv4n2P0mXPzzMT/gvNMfKmp5//MGbjmnVL1SIiIiY1R6hKn3H4e3N8DKv4XFf+V1NQlxeqV2MGPiBL52snqlRERExqrUD1OdA87nnQFr7va6moS9Xl7Ltj0NfOec+WQGUv+fSUREZLxK7b/STQecK5znTYWvPTwuBpxD91ipWZMn8FX1SomIiIxpqRumQu3wX1dDyyG44nHIneJ1RQl7dfsBPqg6zHfOLiXDn7r/RCIiIqlgfHTVDMW+92DvVrj0FzBtsdfVJMxay32vVjB7cg6XLYu9BaKIiIiMNakbpmavhO+8BxPH14Uuf/9xDR/ubeRHX1usXikREZFxILX/Wo+zIGWt5b7NO5g7JYfLlqpXSkREZDxI7TA1zrz8UQ0f72/kO+eUElCvlIiIyLigv9hjRCRiuW9zBUcX5XLJkulelyMiIiIJUpgaI176qJpPqoP8j9XqlRIRERlP9Fd7DIhELD/bvINjinO56CT1SomIiIwnClNjwKYP91Ne4/RK+X3G63JERERkEBSmPBZ2e6Xml+Rx4WL1SomIiIw3ClMee+Ev+9lxoInvqldKRERkXFKY8pDTK1XBsVPzuODEaV6XIyIiIkOgMOWh332wj09rj/Dd1cfiU6+UiIjIuKQw5ZFQOMLPNu9gwVH5nHfCUV6XIyIiIkOkMOWRjdv2sfPgEW79Uql6pURERMYxhSkPhMIR/u3VHSycVsDaReqVEhERGc8UpjzwXNk+dtU1q1dKREQkBShMjbJQOMLPX9vB8dMLWLtoqtfliIiIyDApTI2yZ9/fy+66Zm790rEYo14pERGR8U5hahR1uL1SJ84o5EsLS7wuR0RERJJAYWoUPfteFXvqW7j1S6XqlRIREUkRClOjpD0U4d9ereSkmYWcs0C9UiIiIqlCYWqUPPNuFXsbWrh1jcZKiYiIpBKFqVHQHorwwH9XsmTWRM46ttjrckRERCSJFKZGwX9t3cPehhZuU6+UiIhIylGYGmFtoTAP/Hcly2ZP5IzSIq/LERERkSRTmBph//XOHvYfblWvlIiISIpSmBpBrR1hHvjvT1k+ZxKr5qtXSkREJBUpTI2gp97ZQ3WjeqVERERSmcLUCHF6pSpZMXcypx0zxetyREREZIQoTI2QX/35cw4E27h1ja52LiIiksoUpkZAa0eYX/zhU1YePZnTjtFYKRERkVSmMDUCHntrN7XBNm770rFelyIiIiIjTGEqyVrawzz4h52cdswUvnC0xkqJiIikuoTClDHmXGNMuTGm0hhze5z1hcaY540x24wxHxljrk1+qePDY2/t5mBTG7etUa+UiIhIOhgwTBlj/MADwHnAIuBKY8yimGZ/C3xsrT0JOAv4sTEmM8m1jnnN7SEe/MOnrJpfxClzJ3tdjoiIiIyCRHqmVgCV1tqd1tp24Engkpg2Fsg3zmlreUA9EEpqpePAo3/aTd2Rdm5bU+p1KSIiIjJKEglTM4A9UfNV7rJo9wMLgX3AX4DvWmsjSalwnDjSFmLDlp18sbSIk+eoV0pERCRdJBKm4l0kycbMfxkoA6YDS4D7jTEFvTZkzA3GmK3GmK21tbWDLHVse+RPu6g/0q6xUiIiImkmkTBVBcyKmp+J0wMV7VrgWeuoBD4DFsRuyFq7wVq73Fq7vLi4eKg1jzlNbq/UmccWs2z2JK/LERERkVGUSJh6Byg1xsxzB5VfAWyMafM5sBrAGDMVOA7YmcxCx7Jntu6hoblDvVIiIiJpKDBQA2ttyBhzC/Ay4AcestZ+ZIy5yV3/IPAvwMPGmL/gHBb8B2vtwRGse0zZVnWYaYXZLJk10etSREREZJQNGKYArLWbgE0xyx6Mmt4HrE1uaeNHeXWQY6fme12GiIiIeEBXQB+mUDhCZW0Txx2lMCUiIpKOFKaGaXd9M+2hCKUleV6XIiIiIh5QmBqmiuoggHqmRERE0pTC1DBV1DRhDMxXz5SIiEhaUpgapoqaILMn55CTmdBYfhEREUkxClPDVF6jM/lERETSmcLUMLSFwnx28AjHKUyJiIikLYWpYdhZe4RwxHKsBp+LiIikLYWpYaiocc/kU8+UiIhI2lKYGoby6iABn2FeUa7XpYiIiIhHFKaGoaImyLyiXDID2o0iIiLpSilgGMprghovJSIikuYUpoaouT3EnvoWjZcSERFJcwpTQ7SjpglA15gSERFJcwpTQ1Reo3vyiYiIiMLUkFVUB8kK+Jg9OcfrUkRERMRDClNDVF4TpHRqHn6f8boUERER8ZDC1BBV6J58IiIigsLUkDQ0t1PT2KYz+URERERhaigqdCafiIiIuBSmhqDznny6YKeIiIgoTA1BRU2QvKwA0wuzvS5FREREPKYwNQTl1UGOnZqHMTqTT0REJN0pTA2StZaKmqAu1ikiIiKAwtSg1Ta1cai5Q4PPRUREBFCYGrSKaudMPl0WQUREREBhatDKdSafiIiIRFGYGqSK6iBTcjMpysvyuhQREREZAxSmBqnznnwiIiIioDA1KNZadtQENV5KREREuihMDcLehhaOtIc1XkpERES6KEwNQudtZNQzJSIiIp0Upgah3L0sQqnClIiIiLgUpgahoibItMJsCidkeF2KiIiIjBEKU4Pg3JNPvVIiIiLSTWEqQaFwhMraJt2TT0RERHpQmErQ7vpm2kMR9UyJiIhIDwpTCdqhM/lEREQkDoWpBJVXN2EMzC/R1c9FRESkW0JhyhhzrjGm3BhTaYy5Pc76vzfGlLmPD40xYWPM5OSX652KmiCzJ+cwIdPvdSkiIiIyhgwYpowxfuAB4DxgEXClMWZRdBtr7Y+stUustUuAfwT+YK2tH4F6PVNeozP5REREpLdEeqZWAJXW2p3W2nbgSeCSftpfCTyRjOLGirZQmM8OHtF4KREREeklkTA1A9gTNV/lLuvFGJMDnAv8eviljR07a48Qjljdk09ERER6SSRMmTjLbB9tLwLe7OsQnzHmBmPMVmPM1tra2kRr9JzuySciIiJ9SSRMVQGzouZnAvv6aHsF/Rzis9ZusNYut9YuLy4uTrxKj5VXBwn4DPOKcr0uRURERMaYRMLUO0CpMWaeMSYTJzBtjG1kjCkEzgR+m9wSvVdRE+To4lwyA7qShIiIiPQUGKiBtTZkjLkFeBnwAw9Zaz8yxtzkrn/QbXoZ8Iq19siIVeuRipomFs8s9LoMERERGYMGDFMA1tpNwKaYZQ/GzD8MPJyswsaK5vYQn9c387WTZ3pdioiIiIxBOm41gB01TQC6xpSIiIjEpTA1gPLOM/l0WQQRERGJQ2FqABXVQbICPmZPzvG6FBERERmDFKYGUF4TpHRqHn5fvMttiYiISLpTmBpAhe7JJyIiIv1QmOpHQ3M7NY1tuvK5iIiI9Elhqh8VnWfyafC5iIiI9EFhqh+6J5+IiIgMRGGqHxU1QfKzAkwrzPa6FBERERmjFKb6UV7tnMlnjM7kExERkfgUpvpgraWiJqiLdYqIiEi/FKb6UNvUxqHmDl0WQURERPqlMNWHimrnTD4NPhcREZH+KEz1ofOefLosgoiIiPRHYaoPFdVBpuRmUpSX5XUpIiIiMoYpTPWh4oBuIyMiIiIDU5iKw1pLRbXO5BMREZGBKUzFsbehhSPtYUqn5nldioiIiIxxClNx6DYyIiIikiiFqTjK3csilCpMiYiIyAAUpuKoqAkyrTCbwgkZXpciIiIiY5zCVBzl1TqTT0RERBKjMBUjFI5QWdukM/lEREQkIQpTMXbXN9MeiqhnSkRERBKiMBVjh87kExERkUFQmIpRXt2EMTC/RNeYEhERkYEpTMWoqAkye3IOEzL9XpciIiIi44DCVIzyGp3JJyIiIolTmIrSFgrz2cEjGi8lIiIiCVOYirKz9gjhiOVYXRZBREREEqQwFUX35BMREZHBUpiKUl4dJOAzzCvK9boUERERGScUpqJU1AQ5ujiXzIB2i4iIiCRGqSFKRU2TzuQTERGRQVGYcjW3h/i8vlnjpURERGRQFKZcO2qaAChVmBIREZFBUJhylXeeyafLIoiIiMggKEy5KqqDZAV8zJ6c43UpIiIiMo4oTLnKa4KUTs3D7zNelyIiIiLjSEJhyhhzrjGm3BhTaYy5vY82ZxljyowxHxlj/pDcMkdehe7JJyIiIkMQGKiBMcYPPACsAaqAd4wxG621H0e1mQj8L+Bca+3nxpiSEap3RBxu7qCmsU1n8omIiMigJdIztQKotNbutNa2A08Cl8S0+QbwrLX2cwBr7YHkljmyKg44g891Tz4REREZrETC1AxgT9R8lbss2rHAJGPM68aYd40x65JV4Ggor9Y9+URERGRoBjzMB8QbkW3jbOdkYDUwAfiTMeYta21Fjw0ZcwNwA8Ds2bMHX+0IqagJkp8VYFphtteliIiIyDiTSM9UFTAran4msC9Om5estUestQeBLcBJsRuy1m6w1i631i4vLi4eas1JV17tnMlnjM7kExERkcFJJEy9A5QaY+YZYzKBK4CNMW1+C3zRGBMwxuQAXwC2J7fUkWGtpaImqIt1ioiIyJAMeJjPWhsyxtwCvAz4gYestR8ZY25y1z9ord1ujHkJ+ACIAP9urf1wJAtPltqmNg41d+iyCCIiIjIkiYyZwlq7CdgUs+zBmPkfAT9KXmmjo6LauSefBp+LiIjIUKT9FdA778mnyyKIiIjIUKR9mKqoDjIlN5OivCyvSxEREZFxSGHqgG4jIyIiIkOX1mHKWktFtc7kExERkaFL6zC1t6GFI+1h9UyJiIjIkKV1mKroHHw+Nc/jSkRERGS8SuswVe5eFqFUPVMiIiIyRGkdpipqgkwrzKZwQobXpYiIiMg4ldZhqrxaZ/KJiIjI8KRtmApHLJW1TTqTT0RERIYlbcPU7rojtIci6pkSERGRYUnbMNV5Jp/uySciIiLDkbZhqry6CWNgfokuiyAiIiJDl7ZhqqImyJzJOUzI9HtdioiIiIxjaRumymt0Jp+IiIgMX1qGqbZQmM8OHlGYEhERkWFLyzC1s/YI4YjlWF0WQURERIYpLcOUzuQTERGRZEnLMFVeHSTgM8wryvW6FBERERnn0jJMVdQ0cXRxLpmBtPz4IiIikkRpmSYqdCafiIiIJEnahanm9hCf1zdrvJSIiIgkRdqFqR01TQA6k09ERESSIu3CVLnO5BMREZEkSrswVVEdJCvgY9bkHK9LERERkRSQdmGqvCZI6dQ8/D7jdSkiIiKSAtIuTOlMPhEREUmmtApTh5s7qGls03gpERERSZq0ClMVB5zB5zqTT0RERJIlrcJUebXO5BMREZHkSqswVVETJD8rwLTCbK9LERERkRSRVmGqvDrIsUflY4zO5BMREZHkCHhdwGix1lJRE+TcE6Z5XYqIiIxTHR0dVFVV0dra6nUpMkKys7OZOXMmGRkZCb8mbcJUbVMbh5o7OHZqnteliIjIOFVVVUV+fj5z587VUY4UZK2lrq6Oqqoq5s2bl/Dr0uYwX0W1c08+DT4XEZGham1tZcqUKQpSKcoYw5QpUwbd85g2Yarznny6LIKIiAyHglRqG8q/b9qEqR01QabkZlKUl+V1KSIiImPC3LlzOXjwYMJtvvWtb1FSUsIJJ5yQtBoefPBBHn300aRtL57169ezefPmXstff/11LrzwwmFvP23GTJXrNjIiIiLDcs0113DLLbewbt26pG3zpptuStq2+nL33XeP6PbTomfKWktFdZDjdIhPRETGsV27drFgwQKuv/56TjjhBK666io2b97M6aefTmlpKW+//TYA9fX1XHrppSxevJiVK1fywQcfAFBXV8fatWtZunQpN954I9barm0/9thjrFixgiVLlnDjjTcSDod7vf8ZZ5zB5MmTh1z/7bffzqJFi1i8eDHf+973ALjzzjv513/9VwDeeecdFi9ezKmnnsrf//3fd/WAPfzww1x66aVcdNFFzJs3j/vvv5+f/OQnLF26lJUrV1JfXw9AWVkZK1euZPHixVx22WUcOnQIcELgM888A8BLL73EggULWLVqFc8+++yQP0u0hHqmjDHnAj8D/MC/W2vvjVl/FvBb4DN30bPW2pGNgYOwt6GFI+1h9UyJiEjS3PX8R3y8rzGp21w0vYA7Ljq+3zaVlZU8/fTTbNiwgVNOOYVf/epXvPHGG2zcuJEf/OAHPPfcc9xxxx0sXbqU5557jtdee41169ZRVlbGXXfdxapVq1i/fj0vvPACGzZsAGD79u089dRTvPnmm2RkZHDzzTfz+OOPJ7UHqr6+nt/85jd88sknGGNoaGjo1ebaa69lw4YNnHbaadx+++091n344Ye8//77tLa2Mn/+fH74wx/y/vvvc9ttt/Hoo49y6623sm7dOn7+859z5plnsn79eu666y7uu+++rm20trby7W9/m9dee4358+dz+eWXJ+WzDdgzZYzxAw8A5wGLgCuNMYviNP2jtXaJ+xgzQQqcK58DHHeULosgIiLj27x58zjxxBPx+Xwcf/zxrF69GmMMJ554Irt27QLgjTfe4K//+q8BOOecc6irq+Pw4cNs2bKFb37zmwBccMEFTJo0CYBXX32Vd999l1NOOYUlS5bw6quvsnPnzqTWXVBQQHZ2Ntdffz3PPvssOTk5PdY3NDQQDAY57bTTAPjGN77RY/3ZZ59Nfn4+xcXFFBYWctFFFwF0fe7Dhw/T0NDAmWeeCcDVV1/Nli1bemzjk08+Yd68eZSWlmKM6doXw5VIz9QKoNJauxPAGPMkcAnwcVIqGAXl7mURStUzJSIiSTJQD9JIycrqPpHK5/N1zft8PkKhEECPw3edOs9Si3e2mrWWq6++mnvuuWdYtYXDYU4++WQALr744h5jlQKBAG+//TavvvoqTz75JPfffz+vvfZajxr6k8jnTsRInI2ZyJipGcCeqPkqd1msU40x24wxLxpjvPmG9aGiJsi0wmwKshO/mqmIiMh4dcYZZ/D4448DzhlrRUVFFBQU9Fj+4osvdo0pWr16Nc888wwHDhwAnENyu3fvHvT7+v1+ysrKKCsr6zXou6mpicOHD3P++edz3333UVZW1mP9pEmTyM/P56233gLgySefHNR7FxYWMmnSJP74xz8C8Mtf/rKrl6rTggUL+Oyzz/j0008BeOKJJwb1Hn1JpGcqXoSLjY/vAXOstU3GmPOB54DSXhsy5gbgBoDZs2cPrtJhKK/WmXwiIpI+7rzzTq699loWL15MTk4OjzzyCAB33HEHV155JcuWLePMM8/s+lu8aNEivv/977N27VoikQgZGRk88MADzJkzp8d2r7zySl5//XUOHjzIzJkzueuuu7juuusSqikYDHLJJZfQ2tqKtZaf/vSnvdr8x3/8B9/+9rfJzc3lrLPOorCwcFCf+5FHHuGmm26iubmZo48+mv/8z//ssT47O5sNGzZwwQUXUFRUxKpVq/jwww8H9R7xmIG61YwxpwJ3Wmu/7M7/I4C1ts++QGPMLmC5tbbPi1csX77cbt26dSg1D0o4Ylm4/iWuOW0u/3T+whF/PxERSV3bt29n4UL9LRkpTU1N5OU545vvvfde9u/fz89+9rNRryPev7Mx5l1r7fJ47RPpmXoHKDXGzAP2AlcAPUaFGWOOAmqstdYYswLn8GHdEOpPut11R2gPRdQzJSIiMsa98MIL3HPPPYRCIebMmcPDDz/sdUkJGTBMWWtDxphbgJdxLo3wkLX2I2PMTe76B4GvAX9jjAkBLcAVdqAur1HSdSafwpSIiMiYdvnllyftcgWjKaHrTFlrNwGbYpY9GDV9P3B/cktLjvLqJoyB+SW6LIKIiIgkX8pfAb2iJsicyTlMyPR7XYqIiIikoJQPU7onn4iIiIyklA5TbaEwnx08onvyiYiIyIhJ6TC1s/YI4YjVlc9FRETimDt3LgcP9nkVox5t9uzZw9lnn83ChQs5/vjjR/SSBWeddRajcfmkZEloAPp4pTP5REREkiMQCPDjH/+YZcuWEQwGOfnkk1mzZg2LFsW7Xe/AwuEwfn9qjGdO6Z6p8uogAZ9hXlGu16WIiIgM265du1iwYAHXX389J5xwAldddRWbN2/m9NNPp7S0lLfffhtwbgdz6aWXsnjxYlauXMkHH3wAQF1dHWvXrmXp0qXceOONPe6H99hjj7FixQqWLFnCjTfeSDgc7vHe06ZNY9myZQDk5+ezcOFC9u7dO6j6586dy913382qVat4+umneeWVVzj11FNZtmwZX//612lqaur1ms6LeAI888wzXHPNNYN6z9GQ4j1TTRxdnEtmIKUzo4iIeOHF26H6L8nd5lEnwnn39tuksrKSp59+mg0bNnDKKafwq1/9ijfeeIONGzfygx/8gOeee4477riDpUuX8txzz/Haa6+xbt06ysrKuOuuu1i1ahXr16/nhRdeYMOGDYBzxe+nnnqKN998k4yMDG6++WYef/xx1q1bF7eGXbt28f777/OFL3xh0B8xOzubN954g4MHD/KVr3yFzZs3k5ubyw9/+EN+8pOfsH79+kFv02spHqaCLJ45uPv6iIiIjGXz5s3jxBNPBOD4449n9erVGGM48cQT2bVrFwBvvPEGv/71rwE455xzqKur4/Dhw2zZsoVnn30WgAsuuIBJkyYB8Oqrr/Luu+9yyimnANDS0kJJSUnc929qauKrX/0q9913HwUFBYOuv/OinG+99RYff/wxp59+OgDt7e2ceuqpg97eWJCyYaq5PcTn9c18/eSZXpciIiKpaIAepJGSlZXVNe3z+brmfT4foVAIgHg3ITHG9HiOZq3l6quv5p57+rztLgAdHR189atf5aqrruIrX/lKr/XhcJiTTz4ZgIsvvpi77767V5vc3Nyu91yzZg1PPPFEv+8ZXW9ra2u/bb2Ssse/dtQ4x12P1WURREQkzZxxxhk8/vjjALz++usUFRVRUFDQY/mLL77IoUOHAFi9ejXPPPMMBw4cAJwxV7t37+6xTWst1113HQsXLuTv/u7v4r6v3++nrKyMsrKyuEEq2sqVK3nzzTeprKwEoLm5mYqKil7tpk6dyvbt24lEIvzmN78ZxF4YPSkbphZMy+d331nFyqOneF2KiIjIqLrzzjvZunUrixcv5vbbb+eRRx4B4I477mDLli0sW7aMV155hdmzZwOwaNEivv/977N27VoWL17MmjVr2L9/f49tvvnmm/zyl7/ktddeY8mSJSxZsoRNmzb1eu9EFRcX8/DDD3PllVd2DZT/5JNPerW79957ufDCCznnnHOYNm3akN9vJBmv7ke8fPlyO56uISEiIrJ9+3YWLlzodRkywuL9Oxtj3rXWLo/XPmV7pkRERERGg8KUiIiIyDAoTImIiIgMg8KUiIjIIHg11lhGx1D+fRWmREREEpSdnU1dXZ0CVYqy1lJXV0d2dvagXpeyF+0UERFJtpkzZ1JVVUVtba3XpcgIyc7OZubMwV3wW2FKREQkQRkZGcybN8/rMmSM0WE+ERERkWFQmBIREREZBoUpERERkWHw7HYyxphaYPeADYevCDg4Cu8z1mk/dNO+6KZ90U37wqH90E37opv2Bcyx1hbHW+FZmBotxpitfd1LJ51oP3TTvuimfdFN+8Kh/dBN+6Kb9kX/dJhPREREZBgUpkRERESGIR3C1AavCxgjtB+6aV90077opn3h0H7opn3RTfuiHyk/ZkpERERkJKVDz5SIiIjIiEmJMGWMOdcYU26MqTTG3B5nvTHG/Ju7/gNjzDIv6hxpxphZxpj/NsZsN8Z8ZIz5bpw2ZxljDhtjytzHei9qHQ3GmF3GmL+4n3NrnPXp8r04Lurfu8wY02iMuTWmTUp+L4wxDxljDhhjPoxaNtkY83tjzA73eVIfr+3398p408e++JEx5hP3+/8bY8zEPl7b78/SeNPHvrjTGLM36mfg/D5emw7fi6ei9sMuY0xZH69Nqe/FsFhrx/UD8AOfAkcDmcA2YFFMm/OBFwEDrAT+7HXdI7QvpgHL3Ol8oCLOvjgL+J3XtY7S/tgFFPWzPi2+FzGf2Q9U41wvJeW/F8AZwDLgw6hl/z9wuzt9O/DDPvZTv79Xxtujj32xFgi40z+Mty/cdf3+LI23Rx/74k7gewO8Li2+FzHrfwysT4fvxXAeqdAztQKotNbutNa2A08Cl8S0uQR41DreAiYaY6aNdqEjzVq731r7njsdBLYDM7ytakxLi+9FjNXAp9ba0bhgruestVuA+pjFlwCPuNOPAJfGeWkiv1fGlXj7wlr7irU25M6+Bcwc9cI80Mf3IhFp8b3oZIwxwF8BT4xqUeNQKoSpGcCeqPkqegeIRNqkFGPMXGAp8Oc4q081xmwzxrxojDl+dCsbVRZ4xRjzrjHmhjjr0+57AVxB378Y0+V7MdVaux+c/4AAJXHapON341s4PbXxDPSzlCpucQ95PtTH4d90+158Eaix1u7oY326fC8GlAphysRZFnuKYiJtUoYxJg/4NXCrtbYxZvV7OId4TgJ+Djw3yuWNptOttcuA84C/NcacEbM+3b4XmcDFwNNxVqfT9yIR6fbd+GcgBDzeR5OBfpZSwS+AY4AlwH6cw1ux0up7AVxJ/71S6fC9SEgqhKkqYFbU/Exg3xDapARjTAZOkHrcWvts7HprbaO1tsmd3gRkGGOKRrnMUWGt3ec+HwB+g9NFHy1tvheu84D3rLU1sSvS6XsB1HQeznWfD8RpkzbfDWPM1cCFwFXWHQgTK4GfpXHPWltjrQ1bayPA/yH+Z0yn70UA+ArwVF9t0uF7kahUCFPvAKXGmHnu/7yvADbGtNkIrHPP3loJHO7s5k8l7vHt/wC2W2t/0kebo9x2GGNW4HwH6kavytFhjMk1xuR3TuMMtP0wpllafC+i9Pm/zHT5Xrg2Ale701cDv43TJpHfK+OeMeZc4B+Ai621zX20SeRnadyLGS95GfE/Y1p8L1xfAj6x1lbFW5ku34uEeT0CPhkPnLOyKnDOsvhnd9lNwE3utAEecNf/BVjudc0jtB9W4XQ5fwCUuY/zY/bFLcBHOGehvAWc5nXdI7QvjnY/4zb386bt98L9rDk44agwalnKfy9wwuN+oAOnV+E6YArwKrDDfZ7stp0ObIp6ba/fK+P50ce+qMQZA9T5++LB2H3R18/SeH70sS9+6f4e+AAnIE1L1++Fu/zhzt8PUW1T+nsxnIeugC4iIiIyDKlwmE9ERETEMwpTIiIiIsOgMCUiIiIyDApTIiIiIsOgMCUiIiIyDApTIiIiIsOgMCUiIiIyDApTIiIiIsPw/wAQKmqqxIA1fgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "#range 범위 : epochs 갯수\n",
    "plt.plot(range(20), history1.history['accuracy'], label = 'model1 - sigmoid')\n",
    "plt.plot(range(20), history2.history['accuracy'], label = 'model2 - relu')\n",
    "\n",
    "plt.legend()#범례표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47001666,\n",
       " 0.7942167,\n",
       " 0.9044,\n",
       " 0.9235333,\n",
       " 0.93268335,\n",
       " 0.93993336,\n",
       " 0.94528335,\n",
       " 0.9504833,\n",
       " 0.95435,\n",
       " 0.9583667,\n",
       " 0.96165,\n",
       " 0.96323335,\n",
       " 0.96673334,\n",
       " 0.96811664,\n",
       " 0.96935,\n",
       " 0.9713167,\n",
       " 0.97245,\n",
       " 0.97388333,\n",
       " 0.97525,\n",
       " 0.97546667]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model1에서 epochs을 돌린결과가 들어있음\n",
    "#history라는 함수를 통해서 accuracy\n",
    "\n",
    "history1.history['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jpg>컬러사진\n",
    "#gif>흑백사진\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###직접 그린 그림 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as pimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fa1f534710>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_num33 = pimg.open('num33.gif')\n",
    "\n",
    "plt.imshow(img_num33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAMAAABF0y+mAAADAFBMVEUAAAAAADMAAGYAAJkAAMwAAP8AKwAAKzMAK2YAK5kAK8wAK/8AVQAAVTMAVWYAVZkAVcwAVf8AgAAAgDMAgGYAgJkAgMwAgP8AqgAAqjMAqmYAqpkAqswAqv8A1QAA1TMA1WYA1ZkA1cwA1f8A/wAA/zMA/2YA/5kA/8wA//8zAAAzADMzAGYzAJkzAMwzAP8zKwAzKzMzK2YzK5kzK8wzK/8zVQAzVTMzVWYzVZkzVcwzVf8zgAAzgDMzgGYzgJkzgMwzgP8zqgAzqjMzqmYzqpkzqswzqv8z1QAz1TMz1WYz1Zkz1cwz1f8z/wAz/zMz/2Yz/5kz/8wz//9mAABmADNmAGZmAJlmAMxmAP9mKwBmKzNmK2ZmK5lmK8xmK/9mVQBmVTNmVWZmVZlmVcxmVf9mgABmgDNmgGZmgJlmgMxmgP9mqgBmqjNmqmZmqplmqsxmqv9m1QBm1TNm1WZm1Zlm1cxm1f9m/wBm/zNm/2Zm/5lm/8xm//+ZAACZADOZAGaZAJmZAMyZAP+ZKwCZKzOZK2aZK5mZK8yZK/+ZVQCZVTOZVWaZVZmZVcyZVf+ZgACZgDOZgGaZgJmZgMyZgP+ZqgCZqjOZqmaZqpmZqsyZqv+Z1QCZ1TOZ1WaZ1ZmZ1cyZ1f+Z/wCZ/zOZ/2aZ/5mZ/8yZ///MAADMADPMAGbMAJnMAMzMAP/MKwDMKzPMK2bMK5nMK8zMK//MVQDMVTPMVWbMVZnMVczMVf/MgADMgDPMgGbMgJnMgMzMgP/MqgDMqjPMqmbMqpnMqszMqv/M1QDM1TPM1WbM1ZnM1czM1f/M/wDM/zPM/2bM/5nM/8zM////AAD/ADP/AGb/AJn/AMz/AP//KwD/KzP/K2b/K5n/K8z/K///VQD/VTP/VWb/VZn/Vcz/Vf//gAD/gDP/gGb/gJn/gMz/gP//qgD/qjP/qmb/qpn/qsz/qv//1QD/1TP/1Wb/1Zn/1cz/1f///wD//zP//2b//5n//8z///8AAAAAAAAAAAAAAADZ9vIoAAAA/XRSTlP///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////8A9k80AwAAAIhJREFUeJy90jEOgCAMBdBOXMaJyft08j6deh8mr8RiJFJK+TFOdpDh2dD+QPWl6Ctqpk0hlkStsq54ktWKT9v9UYTcf1pQ6GgnI3RXM8I+L9wz2by6olDUcCdPu4JsxVpR8NaKUAIWn3cNOEUKcOgRUIa2oHgaSMnXHvZ0+Vj0bhWO9v31/YUXRHFivV/sSnQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.GifImagePlugin.GifImageFile image mode=P size=28x28 at 0x1F8657441D0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#28*28 2차원 데이터> 784의 1차원 데이터로 변환\n",
    "#0~255사이의 픽셀값 > 0~1 사이의 픽셀 값\n",
    "img_num33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 타입을 넘파이 배열로 변환\n",
    "num33 = np.array(img_num33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존에는 흰색>0 검은색 > 255\n",
    "#지금은 흰색이 >255 검은색 >0\n",
    "\n",
    "num33 = 255 - num33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num33 = num33.reshape(1,784)\n",
    "num33 = num33.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, 97,  0,  0,  0,  0,  0,  1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#예측하기\n",
    "#10개의 값이 출력\n",
    "# 0:0%\n",
    "#1 :0%\n",
    "#2:0%\n",
    "#3:0%\n",
    "#4:99%\n",
    "(model2.predict(num33)*100).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict_classes(num33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##딥러닝 모델 저장 -파일형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('./model/model_handnum1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('./model/model_handnum1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0391 - accuracy: 0.9881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03905361194554716, 0.98808336]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영선씨의 모델 가져오기\n",
    "model_youngsun = load_model('./model/model_youngsun.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0010 - accuracy: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.001041066051255196, 0.99965]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_youngsun.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##딥러닝 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model():\n",
    "    model2 = Sequential()\n",
    "\n",
    "    model2.add(Dense(units = 32, input_dim = 784, activation = 'relu'))\n",
    "    model2.add(Dense(units = 64, activation = 'relu'))\n",
    "    model2.add(Dense(units = 128,  activation = 'relu'))\n",
    "    model2.add(Dense(units = 64,  activation = 'relu'))\n",
    "    model2.add(Dense(units = 32,  activation = 'relu'))\n",
    "\n",
    "    #출력층의 활성화 함수 : relu\n",
    "    model2.add(Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "    model2.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.3297 - accuracy: 0.8985\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.1722 - accuracy: 0.9491\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.1390 - accuracy: 0.9596\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.1185 - accuracy: 0.9647\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.1059 - accuracy: 0.9683\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0933 - accuracy: 0.9721\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0848 - accuracy: 0.9747\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0771 - accuracy: 0.9766\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0718 - accuracy: 0.9783\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0678 - accuracy: 0.9798\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0634 - accuracy: 0.9814\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0586 - accuracy: 0.9825\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0585 - accuracy: 0.9831\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0551 - accuracy: 0.9830\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0511 - accuracy: 0.9844\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0491 - accuracy: 0.9852\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0511 - accuracy: 0.9845\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0469 - accuracy: 0.9863\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0465 - accuracy: 0.9860\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0433 - accuracy: 0.9869\n",
      "12000/12000 [==============================] - 1s 92us/sample - loss: 0.1807 - accuracy: 0.9667\n",
      "Train on 48000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.3079 - accuracy: 0.9066\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.1603 - accuracy: 0.9529\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.1278 - accuracy: 0.9618\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.1088 - accuracy: 0.9678\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.0969 - accuracy: 0.9716\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.0869 - accuracy: 0.9741\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0778 - accuracy: 0.9772\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0709 - accuracy: 0.9788\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0665 - accuracy: 0.9801\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.0609 - accuracy: 0.9819\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0561 - accuracy: 0.9834\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0527 - accuracy: 0.9845\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0528 - accuracy: 0.9849\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0492 - accuracy: 0.9854\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0465 - accuracy: 0.9861\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0446 - accuracy: 0.9866\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0433 - accuracy: 0.9868\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0430 - accuracy: 0.9876\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 130us/sample - loss: 0.0415 - accuracy: 0.9880\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0405 - accuracy: 0.9884\n",
      "12000/12000 [==============================] - 1s 91us/sample - loss: 0.1950 - accuracy: 0.9663\n",
      "Train on 48000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 130us/sample - loss: 0.3226 - accuracy: 0.9016\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.1718 - accuracy: 0.9498\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.1367 - accuracy: 0.9601\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.1162 - accuracy: 0.9645\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.1026 - accuracy: 0.9687\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0897 - accuracy: 0.9735\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0824 - accuracy: 0.9762\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0737 - accuracy: 0.9780\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0685 - accuracy: 0.9784\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0667 - accuracy: 0.9796\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0591 - accuracy: 0.9820\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0575 - accuracy: 0.9829\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0583 - accuracy: 0.9824\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0515 - accuracy: 0.9846\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0501 - accuracy: 0.9847\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0479 - accuracy: 0.9859\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0451 - accuracy: 0.9864\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0453 - accuracy: 0.9869\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0417 - accuracy: 0.9876\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0413 - accuracy: 0.9880\n",
      "12000/12000 [==============================] - 1s 92us/sample - loss: 0.1764 - accuracy: 0.9674\n",
      "Train on 48000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.3212 - accuracy: 0.9039\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.1665 - accuracy: 0.9504\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.1328 - accuracy: 0.9611\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.1120 - accuracy: 0.9670\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0973 - accuracy: 0.9704\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0880 - accuracy: 0.9741\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0780 - accuracy: 0.9768\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 121us/sample - loss: 0.0725 - accuracy: 0.9779\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0668 - accuracy: 0.9808\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0586 - accuracy: 0.9815\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0598 - accuracy: 0.9819\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0561 - accuracy: 0.9836\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0549 - accuracy: 0.9837\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0500 - accuracy: 0.9852\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0464 - accuracy: 0.9858\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0471 - accuracy: 0.9859\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0460 - accuracy: 0.9866\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0435 - accuracy: 0.9868\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.0424 - accuracy: 0.9875\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0393 - accuracy: 0.9882\n",
      "12000/12000 [==============================] - 1s 92us/sample - loss: 0.1908 - accuracy: 0.9689\n",
      "Train on 48000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.3112 - accuracy: 0.9046\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.1680 - accuracy: 0.9503\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.1344 - accuracy: 0.9605\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.1123 - accuracy: 0.9670\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0993 - accuracy: 0.9712\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0868 - accuracy: 0.9742\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0798 - accuracy: 0.9755\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0745 - accuracy: 0.9772\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0695 - accuracy: 0.9791\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0628 - accuracy: 0.9817\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0610 - accuracy: 0.9811\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.0589 - accuracy: 0.9830\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 131us/sample - loss: 0.0539 - accuracy: 0.9841\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0524 - accuracy: 0.9850\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0484 - accuracy: 0.9865\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.0475 - accuracy: 0.9859\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0470 - accuracy: 0.9866\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0462 - accuracy: 0.9870\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0434 - accuracy: 0.9874\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.0422 - accuracy: 0.9876\n",
      "12000/12000 [==============================] - 1s 90us/sample - loss: 0.1676 - accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#KerasClassifier(딥러닝 모델 함수, epochs, batch_size)\n",
    "model3 = KerasClassifier(build_fn = deep_model, epochs = 20, batch_size = 10)\n",
    "\n",
    "#cross_val_score(모델, 학습데이터, 정답데이터, cv = KFold를 사용한 변수)\n",
    "\n",
    "#몇 개로 어떻게 구분할 것인지\n",
    "# KFold(n_split = 몇 개로 나눌것인지, shuffle, 데이터를 섞을 건지, random_state)\n",
    "fold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "# cross_val_score(모델, 학습데이터, 정답데이터, cv = KFold를 사용한 변수)\n",
    "score = cross_val_score(model3, X_train, y_train, cv = fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666664, 0.96625   , 0.96741664, 0.96891665, 0.96883333])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##베스트 모델 찾아서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#모델을 저장할 폴더명\n",
    "\n",
    "MODEL_FOLDER = './model'\n",
    "# 해당 폴더가 없다면 해당폴더를 생성\n",
    "if not os.path.exists(MODEL_FOLDER) :\n",
    "    os.mkdir(MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장할 파일 명 설정\n",
    "#{epoch:04d} : 반복수를 4자리로 표시\n",
    "#{val_accuracy:.4f} : 검증 정확도를 소수점 4째자리까지 표시\n",
    "\n",
    "modelpath = MODEL_FOLDER + '/handnum-{epoch:04d}-{val_accuracy:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델을 찾아서 만들어둔 파일 명으로 저장\n",
    "#ModelCheckpoint(filepath = 파일경로, monitor = 기준값, save_best_only = True)\n",
    "#save_best_only = True : 더 나은 결과값만 저장\n",
    "mc = ModelCheckpoint(filepath = modelpath,\n",
    "                    monitor = 'val_accuracy',\n",
    "                    save_best_only = True,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EarlyStopping(monitor, patience = 기다리는 횟수)\n",
    "#patience = 20 : monitor 에 적은 기준에 따라 학습 결과가 더 나아지지 않더라도\n",
    "#20은 돌려보겠다\n",
    "es = EarlyStopping(monitor = 'val_accuracy',\n",
    "                  patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40199 samples, validate on 19801 samples\n",
      "Epoch 1/1000\n",
      "39150/40199 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8656\n",
      "Epoch 00001: val_accuracy improved from 0.10878 to 0.92990, saving model to ./model/handnum-0001-0.9299.hdf5\n",
      "40199/40199 [==============================] - 2s 56us/sample - loss: 0.4246 - accuracy: 0.8675 - val_loss: 0.2312 - val_accuracy: 0.9299\n",
      "Epoch 2/1000\n",
      "39750/40199 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9446\n",
      "Epoch 00002: val_accuracy improved from 0.92990 to 0.95152, saving model to ./model/handnum-0002-0.9515.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.1831 - accuracy: 0.9449 - val_loss: 0.1616 - val_accuracy: 0.9515\n",
      "Epoch 3/1000\n",
      "39400/40199 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.9570\n",
      "Epoch 00003: val_accuracy improved from 0.95152 to 0.95329, saving model to ./model/handnum-0003-0.9533.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.1399 - accuracy: 0.9570 - val_loss: 0.1521 - val_accuracy: 0.9533\n",
      "Epoch 4/1000\n",
      "39700/40199 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9653\n",
      "Epoch 00004: val_accuracy did not improve from 0.95329\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.1136 - accuracy: 0.9653 - val_loss: 0.1661 - val_accuracy: 0.9491\n",
      "Epoch 5/1000\n",
      "39050/40199 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9701\n",
      "Epoch 00005: val_accuracy improved from 0.95329 to 0.95970, saving model to ./model/handnum-0005-0.9597.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0950 - accuracy: 0.9700 - val_loss: 0.1365 - val_accuracy: 0.9597\n",
      "Epoch 6/1000\n",
      "40150/40199 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9735\n",
      "Epoch 00006: val_accuracy improved from 0.95970 to 0.96429, saving model to ./model/handnum-0006-0.9643.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0831 - accuracy: 0.9736 - val_loss: 0.1240 - val_accuracy: 0.9643\n",
      "Epoch 7/1000\n",
      "38950/40199 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9766\n",
      "Epoch 00007: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0741 - accuracy: 0.9766 - val_loss: 0.1345 - val_accuracy: 0.9625\n",
      "Epoch 8/1000\n",
      "38650/40199 [===========================>..] - ETA: 0s - loss: 0.0652 - accuracy: 0.9791\n",
      "Epoch 00008: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0654 - accuracy: 0.9789 - val_loss: 0.1447 - val_accuracy: 0.9624\n",
      "Epoch 9/1000\n",
      "39300/40199 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9808\n",
      "Epoch 00009: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0579 - accuracy: 0.9810 - val_loss: 0.1291 - val_accuracy: 0.9636\n",
      "Epoch 10/1000\n",
      "40050/40199 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9835\n",
      "Epoch 00010: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0511 - accuracy: 0.9835 - val_loss: 0.1444 - val_accuracy: 0.9625\n",
      "Epoch 11/1000\n",
      "39400/40199 [============================>.] - ETA: 0s - loss: 0.0502 - accuracy: 0.9835\n",
      "Epoch 00011: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0501 - accuracy: 0.9835 - val_loss: 0.1490 - val_accuracy: 0.9614\n",
      "Epoch 12/1000\n",
      "38050/40199 [===========================>..] - ETA: 0s - loss: 0.0421 - accuracy: 0.9863\n",
      "Epoch 00012: val_accuracy did not improve from 0.96429\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0432 - accuracy: 0.9859 - val_loss: 0.1412 - val_accuracy: 0.9640\n",
      "Epoch 13/1000\n",
      "39800/40199 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9861\n",
      "Epoch 00013: val_accuracy improved from 0.96429 to 0.96460, saving model to ./model/handnum-0013-0.9646.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0410 - accuracy: 0.9861 - val_loss: 0.1411 - val_accuracy: 0.9646\n",
      "Epoch 14/1000\n",
      "39550/40199 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9864\n",
      "Epoch 00014: val_accuracy improved from 0.96460 to 0.96561, saving model to ./model/handnum-0014-0.9656.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0397 - accuracy: 0.9864 - val_loss: 0.1371 - val_accuracy: 0.9656\n",
      "Epoch 15/1000\n",
      "40100/40199 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9888\n",
      "Epoch 00015: val_accuracy improved from 0.96561 to 0.96566, saving model to ./model/handnum-0015-0.9657.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0336 - accuracy: 0.9888 - val_loss: 0.1502 - val_accuracy: 0.9657\n",
      "Epoch 16/1000\n",
      "38600/40199 [===========================>..] - ETA: 0s - loss: 0.0352 - accuracy: 0.9881\n",
      "Epoch 00016: val_accuracy did not improve from 0.96566\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0360 - accuracy: 0.9879 - val_loss: 0.1624 - val_accuracy: 0.9613\n",
      "Epoch 17/1000\n",
      "38650/40199 [===========================>..] - ETA: 0s - loss: 0.0303 - accuracy: 0.9903\n",
      "Epoch 00017: val_accuracy improved from 0.96566 to 0.96732, saving model to ./model/handnum-0017-0.9673.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0299 - accuracy: 0.9904 - val_loss: 0.1432 - val_accuracy: 0.9673\n",
      "Epoch 18/1000\n",
      "39900/40199 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9902\n",
      "Epoch 00018: val_accuracy did not improve from 0.96732\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0309 - accuracy: 0.9902 - val_loss: 0.1654 - val_accuracy: 0.9622\n",
      "Epoch 19/1000\n",
      "39150/40199 [============================>.] - ETA: 0s - loss: 0.0255 - accuracy: 0.9915\n",
      "Epoch 00019: val_accuracy did not improve from 0.96732\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.1726 - val_accuracy: 0.9636\n",
      "Epoch 20/1000\n",
      "39200/40199 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 00020: val_accuracy did not improve from 0.96732\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0280 - accuracy: 0.9902 - val_loss: 0.1725 - val_accuracy: 0.9634\n",
      "Epoch 21/1000\n",
      "39550/40199 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9907\n",
      "Epoch 00021: val_accuracy did not improve from 0.96732\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0277 - accuracy: 0.9907 - val_loss: 0.1698 - val_accuracy: 0.9638\n",
      "Epoch 22/1000\n",
      "38800/40199 [===========================>..] - ETA: 0s - loss: 0.0235 - accuracy: 0.9922\n",
      "Epoch 00022: val_accuracy did not improve from 0.96732\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0233 - accuracy: 0.9922 - val_loss: 0.1783 - val_accuracy: 0.9636\n",
      "Epoch 23/1000\n",
      "39250/40199 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9933\n",
      "Epoch 00023: val_accuracy improved from 0.96732 to 0.96738, saving model to ./model/handnum-0023-0.9674.hdf5\n",
      "40199/40199 [==============================] - 2s 45us/sample - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.1565 - val_accuracy: 0.9674\n",
      "Epoch 24/1000\n",
      "38700/40199 [===========================>..] - ETA: 0s - loss: 0.0252 - accuracy: 0.9926\n",
      "Epoch 00024: val_accuracy improved from 0.96738 to 0.96803, saving model to ./model/handnum-0024-0.9680.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.1613 - val_accuracy: 0.9680\n",
      "Epoch 25/1000\n",
      "39150/40199 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9923\n",
      "Epoch 00025: val_accuracy improved from 0.96803 to 0.96818, saving model to ./model/handnum-0025-0.9682.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1553 - val_accuracy: 0.9682\n",
      "Epoch 26/1000\n",
      "39700/40199 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9938\n",
      "Epoch 00026: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 45us/sample - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.1758 - val_accuracy: 0.9653\n",
      "Epoch 27/1000\n",
      "39300/40199 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9931\n",
      "Epoch 00027: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.1947 - val_accuracy: 0.9647\n",
      "Epoch 28/1000\n",
      "39100/40199 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 00028: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.1912 - val_accuracy: 0.9633\n",
      "Epoch 29/1000\n",
      "39950/40199 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9933\n",
      "Epoch 00029: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.1796 - val_accuracy: 0.9652\n",
      "Epoch 30/1000\n",
      "39750/40199 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9956\n",
      "Epoch 00030: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9956 - val_loss: 0.1901 - val_accuracy: 0.9664\n",
      "Epoch 31/1000\n",
      "39450/40199 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9943\n",
      "Epoch 00031: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0190 - accuracy: 0.9943 - val_loss: 0.1766 - val_accuracy: 0.9659\n",
      "Epoch 32/1000\n",
      "39850/40199 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9932\n",
      "Epoch 00032: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0214 - accuracy: 0.9932 - val_loss: 0.1869 - val_accuracy: 0.9656\n",
      "Epoch 33/1000\n",
      "38750/40199 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9953\n",
      "Epoch 00033: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.2014 - val_accuracy: 0.9650\n",
      "Epoch 34/1000\n",
      "39500/40199 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9942\n",
      "Epoch 00034: val_accuracy did not improve from 0.96818\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.1848 - val_accuracy: 0.9658\n",
      "Epoch 35/1000\n",
      "38550/40199 [===========================>..] - ETA: 0s - loss: 0.0118 - accuracy: 0.9965\n",
      "Epoch 00035: val_accuracy improved from 0.96818 to 0.96844, saving model to ./model/handnum-0035-0.9684.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.1889 - val_accuracy: 0.9684\n",
      "Epoch 36/1000\n",
      "39900/40199 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9950\n",
      "Epoch 00036: val_accuracy did not improve from 0.96844\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0170 - accuracy: 0.9950 - val_loss: 0.1782 - val_accuracy: 0.9663\n",
      "Epoch 37/1000\n",
      "39150/40199 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9943\n",
      "Epoch 00037: val_accuracy did not improve from 0.96844\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1716 - val_accuracy: 0.9675\n",
      "Epoch 38/1000\n",
      "39600/40199 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9960\n",
      "Epoch 00038: val_accuracy improved from 0.96844 to 0.96864, saving model to ./model/handnum-0038-0.9686.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.1780 - val_accuracy: 0.9686\n",
      "Epoch 39/1000\n",
      "40150/40199 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9950\n",
      "Epoch 00039: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.1872 - val_accuracy: 0.9653\n",
      "Epoch 40/1000\n",
      "39200/40199 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9961\n",
      "Epoch 00040: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.1890 - val_accuracy: 0.9662\n",
      "Epoch 41/1000\n",
      "38900/40199 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9945\n",
      "Epoch 00041: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0175 - accuracy: 0.9945 - val_loss: 0.1906 - val_accuracy: 0.9655\n",
      "Epoch 42/1000\n",
      "39450/40199 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9946\n",
      "Epoch 00042: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.1817 - val_accuracy: 0.9659\n",
      "Epoch 43/1000\n",
      "38900/40199 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9960\n",
      "Epoch 00043: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 45us/sample - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.1864 - val_accuracy: 0.9669\n",
      "Epoch 44/1000\n",
      "39600/40199 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9962\n",
      "Epoch 00044: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 47us/sample - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.1878 - val_accuracy: 0.9680\n",
      "Epoch 45/1000\n",
      "38900/40199 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 00045: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.1899 - val_accuracy: 0.9664\n",
      "Epoch 46/1000\n",
      "39300/40199 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9963\n",
      "Epoch 00046: val_accuracy did not improve from 0.96864\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.2028 - val_accuracy: 0.9648\n",
      "Epoch 47/1000\n",
      "40150/40199 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9961\n",
      "Epoch 00047: val_accuracy improved from 0.96864 to 0.97020, saving model to ./model/handnum-0047-0.9702.hdf5\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.1870 - val_accuracy: 0.9702\n",
      "Epoch 48/1000\n",
      "38600/40199 [===========================>..] - ETA: 0s - loss: 0.0134 - accuracy: 0.9960\n",
      "Epoch 00048: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 41us/sample - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.1812 - val_accuracy: 0.9683\n",
      "Epoch 49/1000\n",
      "39900/40199 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9966\n",
      "Epoch 00049: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 41us/sample - loss: 0.0121 - accuracy: 0.9966 - val_loss: 0.2254 - val_accuracy: 0.9632\n",
      "Epoch 50/1000\n",
      "39400/40199 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9962\n",
      "Epoch 00050: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0132 - accuracy: 0.9962 - val_loss: 0.1984 - val_accuracy: 0.9686\n",
      "Epoch 51/1000\n",
      "39250/40199 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9961\n",
      "Epoch 00051: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0137 - accuracy: 0.9961 - val_loss: 0.2129 - val_accuracy: 0.9653\n",
      "Epoch 52/1000\n",
      "38950/40199 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9969\n",
      "Epoch 00052: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.2242 - val_accuracy: 0.9647\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38250/40199 [===========================>..] - ETA: 0s - loss: 0.0105 - accuracy: 0.9969\n",
      "Epoch 00053: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0111 - accuracy: 0.9967 - val_loss: 0.2042 - val_accuracy: 0.9675\n",
      "Epoch 54/1000\n",
      "39200/40199 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9963\n",
      "Epoch 00054: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.2187 - val_accuracy: 0.9668\n",
      "Epoch 55/1000\n",
      "39700/40199 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 00055: val_accuracy did not improve from 0.97020\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.2000 - val_accuracy: 0.9693\n",
      "Epoch 56/1000\n",
      "39500/40199 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9983\n",
      "Epoch 00056: val_accuracy improved from 0.97020 to 0.97041, saving model to ./model/handnum-0056-0.9704.hdf5\n",
      "40199/40199 [==============================] - 2s 44us/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.2018 - val_accuracy: 0.9704\n",
      "Epoch 57/1000\n",
      "38700/40199 [===========================>..] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 00057: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9957 - val_loss: 0.1939 - val_accuracy: 0.9674\n",
      "Epoch 58/1000\n",
      "39050/40199 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9974\n",
      "Epoch 00058: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.1973 - val_accuracy: 0.9693\n",
      "Epoch 59/1000\n",
      "39550/40199 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9955\n",
      "Epoch 00059: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.1964 - val_accuracy: 0.9686\n",
      "Epoch 60/1000\n",
      "39550/40199 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9979\n",
      "Epoch 00060: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.2160 - val_accuracy: 0.9689\n",
      "Epoch 61/1000\n",
      "38600/40199 [===========================>..] - ETA: 0s - loss: 0.0139 - accuracy: 0.9962\n",
      "Epoch 00061: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.2304 - val_accuracy: 0.9644\n",
      "Epoch 62/1000\n",
      "40100/40199 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9967\n",
      "Epoch 00062: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.2329 - val_accuracy: 0.9645\n",
      "Epoch 63/1000\n",
      "39900/40199 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9960\n",
      "Epoch 00063: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.2025 - val_accuracy: 0.9677\n",
      "Epoch 64/1000\n",
      "39600/40199 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 00064: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.2088 - val_accuracy: 0.9683\n",
      "Epoch 65/1000\n",
      "39750/40199 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9975\n",
      "Epoch 00065: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.2269 - val_accuracy: 0.9680\n",
      "Epoch 66/1000\n",
      "39350/40199 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9964\n",
      "Epoch 00066: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.2346 - val_accuracy: 0.9671\n",
      "Epoch 67/1000\n",
      "40050/40199 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 00067: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2266 - val_accuracy: 0.9683\n",
      "Epoch 68/1000\n",
      "39950/40199 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9961\n",
      "Epoch 00068: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.2248 - val_accuracy: 0.9640\n",
      "Epoch 69/1000\n",
      "39250/40199 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9973\n",
      "Epoch 00069: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.2833 - val_accuracy: 0.9604\n",
      "Epoch 70/1000\n",
      "38600/40199 [===========================>..] - ETA: 0s - loss: 0.0110 - accuracy: 0.9970\n",
      "Epoch 00070: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.2567 - val_accuracy: 0.9638\n",
      "Epoch 71/1000\n",
      "40100/40199 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 00071: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.2315 - val_accuracy: 0.9659\n",
      "Epoch 72/1000\n",
      "39100/40199 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9969\n",
      "Epoch 00072: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.2258 - val_accuracy: 0.9672\n",
      "Epoch 73/1000\n",
      "39300/40199 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9973\n",
      "Epoch 00073: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 43us/sample - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.2196 - val_accuracy: 0.9697\n",
      "Epoch 74/1000\n",
      "39150/40199 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9978\n",
      "Epoch 00074: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.2385 - val_accuracy: 0.9678\n",
      "Epoch 75/1000\n",
      "39000/40199 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9970\n",
      "Epoch 00075: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.2289 - val_accuracy: 0.9661\n",
      "Epoch 76/1000\n",
      "40100/40199 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9979\n",
      "Epoch 00076: val_accuracy did not improve from 0.97041\n",
      "40199/40199 [==============================] - 2s 42us/sample - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.2107 - val_accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "#학습\n",
    "#validation_split = 0.33:전체 데이터 중에서 33%를 검증데이터로 활용 평가\n",
    "history = model2.fit(X_train, y_train, epochs = 1000, batch_size = 50,\n",
    "                    validation_split = 0.33,\n",
    "                    callbacks = [mc,es]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
